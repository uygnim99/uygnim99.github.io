<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>NeRF 기본 개념 설명 | Mingdarin</title>
<meta name=keywords content="nerf"><meta name=description content="paper: NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis
앞으로 리뷰하고 공부하게 될 논문들의 기본 베이스 개념인 NeRF에 대해서 간단히 알아봅시다.
Neural Radiance Field 논문에서 다루는 view synthesis 테스크는 어떤 물체를 여러 각도로 찍은 사진을 이용하여 새로운 각도에서 물체를 바라본 이미지를 얻어내는 작업입니다.
MLP를 사용해 2D image를 input으로 활용하여 3D object의 color값과 volume density값을 예측하고, 이를 이용해 novel view image를 얻어내는 모델입니다.
논문을 이해하기 위해선 camera model, volume rendering, ray tracing등의 컴퓨터 비전 관련 지식들이 필요하기 때문에, 차후에 하나씩 정리해서 추가해 보도록 하고, 이 글에서는 논문에 쓰인 기법들 위주로 설명해 보도록 하겠습니다."><meta name=author content><link rel=canonical href=https://canonical.url/to/page><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://uygnim99.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://uygnim99.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://uygnim99.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://uygnim99.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://uygnim99.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="NeRF 기본 개념 설명"><meta property="og:description" content="paper: NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis
앞으로 리뷰하고 공부하게 될 논문들의 기본 베이스 개념인 NeRF에 대해서 간단히 알아봅시다.
Neural Radiance Field 논문에서 다루는 view synthesis 테스크는 어떤 물체를 여러 각도로 찍은 사진을 이용하여 새로운 각도에서 물체를 바라본 이미지를 얻어내는 작업입니다.
MLP를 사용해 2D image를 input으로 활용하여 3D object의 color값과 volume density값을 예측하고, 이를 이용해 novel view image를 얻어내는 모델입니다.
논문을 이해하기 위해선 camera model, volume rendering, ray tracing등의 컴퓨터 비전 관련 지식들이 필요하기 때문에, 차후에 하나씩 정리해서 추가해 보도록 하고, 이 글에서는 논문에 쓰인 기법들 위주로 설명해 보도록 하겠습니다."><meta property="og:type" content="article"><meta property="og:url" content="https://uygnim99.github.io/post/nerf/nerf-base/"><meta property="og:image" content="https://uygnim99.github.io/%3Cimage%20path/url%3E"><meta property="article:section" content="post"><meta property="article:published_time" content="2023-12-13T17:30:00+00:00"><meta property="article:modified_time" content="2023-12-13T17:30:00+00:00"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://uygnim99.github.io/%3Cimage%20path/url%3E"><meta name=twitter:title content="NeRF 기본 개념 설명"><meta name=twitter:description content="paper: NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis
앞으로 리뷰하고 공부하게 될 논문들의 기본 베이스 개념인 NeRF에 대해서 간단히 알아봅시다.
Neural Radiance Field 논문에서 다루는 view synthesis 테스크는 어떤 물체를 여러 각도로 찍은 사진을 이용하여 새로운 각도에서 물체를 바라본 이미지를 얻어내는 작업입니다.
MLP를 사용해 2D image를 input으로 활용하여 3D object의 color값과 volume density값을 예측하고, 이를 이용해 novel view image를 얻어내는 모델입니다.
논문을 이해하기 위해선 camera model, volume rendering, ray tracing등의 컴퓨터 비전 관련 지식들이 필요하기 때문에, 차후에 하나씩 정리해서 추가해 보도록 하고, 이 글에서는 논문에 쓰인 기법들 위주로 설명해 보도록 하겠습니다."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://uygnim99.github.io/post/"},{"@type":"ListItem","position":2,"name":"NeRF 기본 개념 설명","item":"https://uygnim99.github.io/post/nerf/nerf-base/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"NeRF 기본 개념 설명","name":"NeRF 기본 개념 설명","description":"paper: NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\n앞으로 리뷰하고 공부하게 될 논문들의 기본 베이스 개념인 NeRF에 대해서 간단히 알아봅시다.\nNeural Radiance Field 논문에서 다루는 view synthesis 테스크는 어떤 물체를 여러 각도로 찍은 사진을 이용하여 새로운 각도에서 물체를 바라본 이미지를 얻어내는 작업입니다.\nMLP를 사용해 2D image를 input으로 활용하여 3D object의 color값과 volume density값을 예측하고, 이를 이용해 novel view image를 얻어내는 모델입니다.\n논문을 이해하기 위해선 camera model, volume rendering, ray tracing등의 컴퓨터 비전 관련 지식들이 필요하기 때문에, 차후에 하나씩 정리해서 추가해 보도록 하고, 이 글에서는 논문에 쓰인 기법들 위주로 설명해 보도록 하겠습니다.","keywords":["nerf"],"articleBody":"paper: NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\n앞으로 리뷰하고 공부하게 될 논문들의 기본 베이스 개념인 NeRF에 대해서 간단히 알아봅시다.\nNeural Radiance Field 논문에서 다루는 view synthesis 테스크는 어떤 물체를 여러 각도로 찍은 사진을 이용하여 새로운 각도에서 물체를 바라본 이미지를 얻어내는 작업입니다.\nMLP를 사용해 2D image를 input으로 활용하여 3D object의 color값과 volume density값을 예측하고, 이를 이용해 novel view image를 얻어내는 모델입니다.\n논문을 이해하기 위해선 camera model, volume rendering, ray tracing등의 컴퓨터 비전 관련 지식들이 필요하기 때문에, 차후에 하나씩 정리해서 추가해 보도록 하고, 이 글에서는 논문에 쓰인 기법들 위주로 설명해 보도록 하겠습니다.\nModel Pipeline please 10트\n","wordCount":"98","inLanguage":"en","image":"https://uygnim99.github.io/%3Cimage%20path/url%3E","datePublished":"2023-12-13T17:30:00Z","dateModified":"2023-12-13T17:30:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://uygnim99.github.io/post/nerf/nerf-base/"},"publisher":{"@type":"Organization","name":"Mingdarin","logo":{"@type":"ImageObject","url":"https://uygnim99.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://uygnim99.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://uygnim99.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://uygnim99.github.io/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://uygnim99.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://uygnim99.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://uygnim99.github.io/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://uygnim99.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://uygnim99.github.io/post/>Posts</a></div><h1 class=post-title>NeRF 기본 개념 설명</h1><div class=post-meta><span title='2023-12-13 17:30:00 +0000 +0000'>Date: December 13, 2023</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#model-pipeline>Model Pipeline</a></li></ul></nav></div></details></div><div class=post-content><p>paper: <a href=https://arxiv.org/abs/2003.08934>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</a></p><p>앞으로 리뷰하고 공부하게 될 논문들의 기본 베이스 개념인 NeRF에 대해서 간단히 알아봅시다.</p><h1 id=neural-radiance-field>Neural Radiance Field<a hidden class=anchor aria-hidden=true href=#neural-radiance-field>#</a></h1><p>논문에서 다루는 view synthesis 테스크는 어떤 물체를 여러 각도로 찍은 사진을 이용하여 새로운 각도에서 물체를 바라본 이미지를 얻어내는 작업입니다.</p><p>MLP를 사용해 2D image를 input으로 활용하여 3D object의 color값과 volume density값을 예측하고, 이를 이용해 novel view image를 얻어내는 모델입니다.</p><p>논문을 이해하기 위해선 camera model, volume rendering, ray tracing등의 컴퓨터 비전 관련 지식들이 필요하기 때문에, 차후에 하나씩 정리해서 추가해 보도록 하고, 이 글에서는 논문에 쓰인 기법들 위주로 설명해 보도록 하겠습니다.</p><h2 id=model-pipeline>Model Pipeline<a hidden class=anchor aria-hidden=true href=#model-pipeline>#</a></h2><p>please 10트</p><p><img loading=lazy src=/images/nerf-base/nerf-pipeline.png alt=NeRF-pipeline></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://uygnim99.github.io/tags/nerf/>nerf</a></li></ul><nav class=paginav><a class=prev href=https://uygnim99.github.io/post/relit/relit-base/><span class=title>« Prev</span><br><span>relit</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://uygnim99.github.io/>Mingdarin</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>