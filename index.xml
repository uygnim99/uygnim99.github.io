<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Mingdarin</title>
    <link>https://uygnim99.github.io/</link>
    <description>Recent content on Mingdarin</description>
    <image>
      <title>Mingdarin</title>
      <url>https://uygnim99.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://uygnim99.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 13 Dec 2023 17:30:00 +0000</lastBuildDate><atom:link href="https://uygnim99.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>relit</title>
      <link>https://uygnim99.github.io/post/relit/relit-base/</link>
      <pubDate>Fri, 15 Sep 2023 11:30:03 +0000</pubDate>
      
      <guid>https://uygnim99.github.io/post/relit/relit-base/</guid>
      <description></description>
    </item>
    
    <item>
      <title>NeRF 기본 개념 설명</title>
      <link>https://uygnim99.github.io/post/nerf/nerf-base/</link>
      <pubDate>Wed, 13 Dec 2023 17:30:00 +0000</pubDate>
      
      <guid>https://uygnim99.github.io/post/nerf/nerf-base/</guid>
      <description>paper: NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis
앞으로 리뷰하고 공부하게 될 논문들의 기본 베이스 개념인 NeRF에 대해서 간단히 알아봅시다.
Neural Radiance Field 논문에서 다루는 view synthesis 테스크는 어떤 물체를 여러 각도로 찍은 사진을 이용하여 새로운 각도에서 물체를 바라본 이미지를 얻어내는 작업입니다.
MLP를 사용해 2D image를 input으로 활용하여 3D object의 color값과 volume density값을 예측하고, 이를 이용해 novel view image를 얻어내는 모델입니다.
논문을 이해하기 위해선 camera model, volume rendering, ray tracing등의 컴퓨터 비전 관련 지식들이 필요하기 때문에, 차후에 하나씩 정리해서 추가해 보도록 하고, 이 글에서는 논문에 쓰인 기법들 위주로 설명해 보도록 하겠습니다.</description>
    </item>
    
    
    
  </channel>
</rss>
